{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cd29f2e",
   "metadata": {},
   "source": [
    "### 1. Getting the data\n",
    "Data can be directly read from a file or it might be necessary to scrap the web."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c6ccdd",
   "metadata": {},
   "source": [
    "### 2. Load the data\n",
    "Of course, this depends on what format it is in: plain text, fixed columns, CSV, XML, HTML, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28712745",
   "metadata": {},
   "source": [
    "### 3. Data Cleaning\n",
    "\n",
    "The most common steps are:\n",
    "\n",
    "+ **Sample the data**. If the amount of raw data is huge, processing all of them may require an extensive amount of processing power which may not be practical.  In this case, it is quite common to sample the input data to reduce the size of data that need to be processed.\n",
    "\n",
    "+ **Impute missing data**. It is quite common that some of the input records are incomplete in the sense that certain fields are missing or have input error.  In a typical tabular data format, we need to validate each record contains the same number of fields and each field contains the data type we expect. In case the record has some fields missing, we have the following choices: \n",
    "    +    Discard the whole record if it is incomplete; \n",
    "    +    Infer the missing value based on the data from other records.  A common approach is to fill the missing data with the average, or the median.\n",
    "    \n",
    "+ **Normalize numeric value**. Normalize data is about transforming numeric data into a uniform range.\n",
    "\n",
    "+ **Reduce dimensionality**. High dimensionality can be a problem for some machine learning methods.  There are two ways to reduce the number of input features.  One is about $removing$ $irrelevant$ input variables, another one is about $removing$ $redundant$ input variables.\n",
    "\n",
    "+ **Add derived features**. In some cases, we may need to compute additional attributes from existing attributes (f.e. converting a geo-location to a zip code, or converting the age to an age group).\n",
    "\n",
    "+ **Discretize numeric value into categories**. Discretize data is about cutting a continuous value into ranges and assigning the numeric with the corresponding bucket of the range it falls on.  For numeric attribute, a common way to generalize it is to discretize it into ranges, which can be either constant width (variable height/frequency) or variable width (constant height).\n",
    "\n",
    "+ **Binarize categorical attributes**. Certain machine learning models only take binary input (or numeric input).  In this case, we need to convert categorical attribute into multiple binary attributes, while each binary attribute corresponds to a particular value of the category. \n",
    "\n",
    "+ **Select, combine, aggregate data**. Designing the form of training data is the most important part of the whole predictive modeling exercise because the accuracy largely depends on whether the input features are structured in an appropriate form that provide strong signals to the learning algorithm. Rather than using the raw data as it is, it is quite common that multiple pieces of raw data need to be combined together, or aggregating multiple raw data records along some dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108a844d",
   "metadata": {},
   "source": [
    "### 4. Building data structures \n",
    "Once you read the data, you usually want to store it in a data structure that lends itself to the analysis you want to do."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d99b1ff",
   "metadata": {},
   "source": [
    "### 5. Divide the data\n",
    "Divide our dataset into the characteristics variables (X) and the target variable to predict (Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e8a146",
   "metadata": {},
   "source": [
    "### 6. Model Trainning\n",
    "Depending on the data available and variable we want to predict we will selesct a different algorithm in order to solve our problem. Some common examples are:\n",
    "+ Linear regresion\n",
    "+ Decisions tree\n",
    "+ Random forest\n",
    "+ Neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca2055c",
   "metadata": {},
   "source": [
    "### 7. Model Validation\n",
    "Check if our trainned model is able to predict our target variable with an acceptable error. Otherwise, we will need to go back to poin 6 and choose another model trying to improve the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bedb10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
